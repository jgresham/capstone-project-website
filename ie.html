<!DOCTYPE html>
<html>
<head>
<title>Information extraction</title>
</head>
<body>
<h1>Information extraction</h1>
<ul>
<li><a href=#introduction>Introduction</a></li>
<li><a href=#named_entity_recognition>Named entity recognition</a></li>
<li><a href=#relation_extraction>Relation extraction</a></li>
</ul>
<a name=introduction />
<h2>Introduction</h2>
<p>Most of the world's information is unstructured. Examples of this include
books, encyclopedic websites, forums, and news articles. While humans are able
to understand and extract information from these sources easily, this is a
difficult problem for computers, and not being able to process these documents
hinders the ability for our computational systems and software to use this
unstructured data. Information extraction techniques allow us to extract
structured information from unstructured text so that computational systems
can understand and use it better.</p>

<h2>Uses in question/answering systems</h2>
<p>Although Watson is able to "understand" unstructured text in a way, many
systems are not able to do so, and in other cases, it's much easier to process
structured text (for example, generating lists, tables, and doing data
analysis).</p>
<p>An alternative to NLP-heavy systems like Watson is creating a database
(in many cases, a domain-specific database such as one related to medical
topics) and creating an interface with which users can query the system. This
interface can be used with NLP technology to allow users to query it with
natural language, but there is an underlying structured database. An example
of this is Google's Knowledge Graph. KG was developed from
<a href="http://www.freebase.com">Freebase</a>, discussed later in this page. Google
allows you to perform a query such as, "How tall is Obama?", and their NLP
technology plus KG will be able to give the user an answer in natural
language.</p>

<a name=named_entity_recognition />
<h2>Named entity recognition</h2>
<p><a href="https://en.wikipedia.org/wiki/Named-entity_recognition">Named entity recognition</a>
is used to identify which tokens in text could
be grounded as entities which are part of world knowledge (for example,
country names, dates, and peoples' names). These can be used not only to
create the basis for a database (one entry per entity), but also for easier
processing of unstructured text. Once a system is relatively sure of which
tokens indicate named entities, it can more easily process the relations
between the entities. Two techniques to named entity recognition are
sequence modeling and grammar-based techniques.</p>
<h3>Grammar-based techniques</h3>
<p>Many named entities fit patterns which are easy to find in text. For
example, most proper nouns start with a capital letter (and for every word in
the noun phrase, all words are capitalized, with a few exceptions). It is
less computationally expensive to write a script which identifies tokens in
a document matching a certain pattern (e.g. capitalized or near a word like
"Inc."), but is more time-intensive on the person constructing the patterns,
because many examples need to be looked at and lots of generalizations need to
be made.</p>
<h3>Sequence modeling</h3>
<p>Sequence modeling is a more general technique used to learn and predict things
about/given the sequence of tokens in a document. Another application of
sequence modeling is part of speech tagging. One fairly simple sequence model
is a
<a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov model.</a></p>
<img src="http://iacs-courses.seas.harvard.edu/courses/am207/blog/hmm.png" width=580 height=213/>
<p>In the image above, the bottom nodes <i>x</i> are hidden nodes. The top
nodes <i>y</i> are observed. In the named entity recognition task, one can
think of the observed nodes as the tokens in a sentence, and the hidden nodes
as the named entity tags. The program building and performing inference on
a hidden Markov model will define a tag set for the possible hidden NER tags.
For example, they might have a tag <i>no entity</i>, and for each kind of
named entity tag (person, location, date, etc.), they will have a <i>begin</i>
and <i>continue</i> tag to indicate the location of the entity (and also be
able to deal with entities next to one another).</p>
<p>In the diagram, you can see that arrows point from each named entity tag,
and each named entity tag is directed towards one token. This is a way of
denoting that each internal state (named entity tag) is dependent on the
previous state, and each token is dependent on the hidden tag at that
index.</p>
<p>This may seem a little odd, because there is one probability distribution over
the whole token vocabulary for each named entity tag. It's supposing that you
are able to predict a most likely token given that you know it's, for example,
not a named entity. The
<a href="https://en.wikipedia.org/wiki/Conditional_random_field">conditional random field</a>
approach is another sequence
modeling approach which doesn't use this assumption, and can perform better
on some tasks. But the hidden Markov model approach is able to achieve high
accuracies for some tasks.</p>
<p>A hidden Markov model is built by counting word and tag frequencies. At the
minimum, it must know probabilities of transitioning from one tag to
another (in which you could learn a pattern such as "place names usually have
more than one token in them"), and it also must know the probabilities of a
word being a particular token given that you know its tag. These both can be
found by counting words and named entity tags (requiring that we have a
dataset of documents in which each word is tagged as a part of speech).</p>
<p>Inference is performed on hidden Markov models using the Viterbi algorithm.
This is a functional programming algorithm in that it performs the same
computation for each iteration (each token), but reuses computations from
earlier indices. It finds the probability of the most probable tag sequence
for the document. A backpropogation table allows the program to find what
that most probable sequence is.</p>
<h3>Packages</h3>
Some packages for named entity recognition include:
<ul>
<li><a href="https://gate.ac.uk/">GATE</a> is uses a GUI (and also has a Java
interface). It supports many languages besides English.</li>
<li><a href="https://cogcomp.cs.illinois.edu/page/software_view/NETagger">
NETagger</a> is from UIUC and allows for tagging of basic NER types (people,
organization, locations, other) and more advanced ones.</li>
<li><a href="https://opennlp.apache.org/">OpenNLP</a> allows for more NLP
tasks such as tokenization, POS tagging, etc.</li>
<li><a href="http://stanfordnlp.github.io/CoreNLP/">Stanford CoreNLP</a> does
more than just named entity recognition (e.g. part of speech tagging, parsing,
lemmatization, etc.). It uses a conditional random field. It is written in
Java but you can use a Python wrapper with it.</li>
</ul>
<a name=relation_extraction />
<h2>Relation extraction</h2>
<p>Once named entities are found, relational databases can be found and built
using the entities and the context in which they appear. For example, some
sentences in the text might indicate who is the leader of some country. The
goal of relation extraction is, once we know which tokens refer to what
entities, to find how they are related by using unstructured text. There are
a few more approaches to relation extraction.</p>
<h3>Supervised approaches</h3>
<p>Like in named entity recognition, we could use grammar-based techniques and
supervised learning. Grammar-based techniques are a little easier because it
might be possible to find simple patterns which cover a lot of examples of a
certain relation. However, these rules are very tough to generalize. In
addition, supervised learning is more difficult with relation extraction than
with NER, because it's most intensive to annotate, and because the sentence
features will be a little more complicated (over the entire sentence, its
n-grams, and where the entities occur, rather than just short sequences of
words).</p>
<h3>Unsupervised approaches</h3>
<p>In addition to grammar-based or supervised techniques, there are several
semi- or unsupervised learning techniques which can be used for relation
extraction. One semi-supervised approach is starting with just a few example
annotated sentences and using them to build a larger system. A drawback to this
approach is that if the system gets off track even a little bit (i.e. if it
makes a mistake, doesn't realize it, and then makes decisions assuming that the
mistake is true), it could produce very odd results.</p>
<p>Another approach is using
previously-constructed databases. This is especially useful when trying to
expand these databases. Two such databases are Freebase and Wikipedia (its
information boxes). Given a set of known facts (relations) in these databases,
and a set of new unannotated sentences, we could generate training data for a
relation extractor by labeling sentences containing two entities in a relation
as having that relation. For example, if we know that Obama is the president
of the US, then we will assume any sentences mentioning both express that
relation. Of course, this assumption isn't always true and there might be a lot
of false positives, but it does allow us to find new rules or conditions which
we might not have thought of as expressing a particular relation.</p>
<h3>Resources</h3>
<ul>
<li>Wikipedia info boxes</li>
<li>Freebase</li>
<li><a href=https://en.wikipedia.org/wiki/Knowledge_Graph">Google's Knowledge Graph</a>
 -- based off of Freebase originally</li>
<li>other tables on the web</li>
</ul>
</body>
</html>
